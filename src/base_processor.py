from pathlib import Path
import json
import time
import re
import pandas as pd
from typing import List, Tuple, Dict, Any
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.docstore.document import Document
import os
import concurrent.futures
from tqdm import tqdm
import hashlib
import pickle


class BaseProcessor:
    def __init__(
        self,
        api_key: str,
        database_dir: Path,
        max_tickets_per_batch: int = 50,
        max_workers: int = None,
        use_cache: bool = True,
    ):
        self.database_dir = database_dir
        self.max_tickets_per_batch = max_tickets_per_batch
        self.total_input_tokens = 0
        self.total_output_tokens = 0
        self.max_workers = max_workers or min(
            os.cpu_count(), 4
        )  # Limita ao n√∫mero de CPUs ou 4, o que for menor
        self.use_cache = use_cache

        # Cria diret√≥rio de cache se n√£o existir
        self.cache_dir = self.database_dir / "cache"
        self.cache_dir.mkdir(exist_ok=True)

        # Inicializa o modelo
        self.llm = ChatGoogleGenerativeAI(
            model="gemini-2.5-flash", temperature=0.3, google_api_key=api_key
        )

    def estimate_tokens(self, text: str) -> int:
        """Estima o n√∫mero de tokens assumindo ~1 token a cada 4 caracteres."""
        return len(text) // 4

    def clean_text(self, text: str) -> str:
        """Limpa o texto removendo ou substituindo caracteres que podem causar problemas."""
        # Remove mensagens padr√£o de atendimento
        text = re.sub(
            r"Estamos te conectando a um especialista humano.*?:?\)\s*", "", text
        )
        text = re.sub(
            r"Ol√°,?\s+[^,]+,\s+sou\s+[^,]+\s+e\s+j√°\s+estou\s+aqui\.?\s*", "", text
        )

        # Remove asteriscos usados para √™nfase
        text = re.sub(r"\*([^\*]+)\*", r"\1", text)

        # Remove caracteres de controle e caracteres especiais problem√°ticos
        text = "".join(char for char in text if ord(char) >= 32 or char == "\n")

        # Substitui aspas tipogr√°ficas por aspas simples
        text = (
            text.replace('"', '"').replace('"', '"').replace(""", "'").replace(""", "'")
        )

        # Trata URLs - substitui por uma representa√ß√£o mais segura
        text = re.sub(
            r"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+",
            lambda m: "[URL: " + m.group(0).split("/")[-1] + "]",
            text,
        )

        # Remove m√∫ltiplas linhas em branco
        text = re.sub(r"\n\s*\n", "\n\n", text)

        # Remove espa√ßos em branco no in√≠cio e fim de cada linha
        text = "\n".join(line.strip() for line in text.split("\n"))

        # Remove linhas vazias no in√≠cio e fim do texto
        text = text.strip()

        # Aplica as substitui√ß√µes existentes
        replacements = {
            "\\": " ",
            "\r": " ",
            "\t": " ",
            "|": " ",
            "\u2028": " ",  # line separator
            "\u2029": " ",  # paragraph separator
            "‚Ä¶": "...",  # retic√™ncias
            "‚Äî": "-",  # travess√£o
            "‚Äì": "-",  # h√≠fen
            '"': '"',  # aspas curvas
            '"': '"',  # aspas curvas
            """: "'",       # aspas simples
            """: "'",  # aspas simples
            "¬¥": "'",  # acento agudo
            "`": "'",  # crase
            "‚Ä¢": "*",  # bullet point
            "‚óã": "*",  # c√≠rculo
            "‚óè": "*",  # c√≠rculo preenchido
            "‚ñ°": "*",  # quadrado
            "‚ñ†": "*",  # quadrado preenchido
            "‚ñ∫": ">",  # seta
            "‚ñº": "v",  # seta
            "‚ñ≤": "^",  # seta
            "‚óÑ": "<",  # seta
            "*[": "[",  # remove asterisco antes de colchete
            "]*": "]",  # remove asterisco depois de colchete
            "‚Äé": "",  # caractere invis√≠vel de formata√ß√£o (LEFT-TO-RIGHT MARK)
            "‚Äè": "",  # caractere invis√≠vel de formata√ß√£o (RIGHT-TO-LEFT MARK)
            "Ô∏è": "",  # variation selector
            "‚É£": "",  # combining enclosing keycap
            "‚≠ê": "*",  # estrela
            "‚ú®": "*",  # brilhos
            "‚úÖ": "v",  # check mark
            "‚ùå": "x",  # x mark
            "‚ùó": "!",  # exclama√ß√£o
            "‚ùì": "?",  # interroga√ß√£o
            "„Äê": "[",  # colchetes estilizados
            "„Äë": "]",  # colchetes estilizados
            "Ôºª": "[",  # colchetes largos
            "ÔºΩ": "]",  # colchetes largos
            "Ôºà": "(",  # par√™nteses estilizados
            "Ôºâ": ")",  # par√™nteses estilizados
            "Ôªø": "",  # BOM (Byte Order Mark)
            "\u200e": "",  # LEFT-TO-RIGHT MARK
            "\u200f": "",  # RIGHT-TO-LEFT MARK
            "\u202a": "",  # LEFT-TO-RIGHT EMBEDDING
            "\u202b": "",  # RIGHT-TO-LEFT EMBEDDING
            "\u202c": "",  # POP DIRECTIONAL FORMATTING
            "\u202d": "",  # LEFT-TO-RIGHT OVERRIDE
            "\u202e": "",  # RIGHT-TO-LEFT OVERRIDE
            "Ôºã": "+",  # FULLWIDTH PLUS
            "Ôºé": ".",  # FULLWIDTH STOP
            "Ôºå": ",",  # FULLWIDTH COMMA
            "Ôºö": ":",  # FULLWIDTH COLON
            "Ôºè": "/",  # FULLWIDTH SOLIDUS
            "Ôºà": "(",  # FULLWIDTH LEFT PARENTHESIS
            "Ôºâ": ")",  # FULLWIDTH RIGHT PARENTHESIS
            "Ôºª": "[",  # FULLWIDTH LEFT SQUARE BRACKET
            "ÔºΩ": "]",  # FULLWIDTH RIGHT SQUARE BRACKET
            "Ôºä": "*",  # FULLWIDTH ASTERISK
            "Ôºø": "_",  # FULLWIDTH LOW LINE
            "ÔΩû": "~",  # FULLWIDTH TILDE
            "ÔºÅ": "!",  # FULLWIDTH EXCLAMATION MARK
            "Ôºü": "?",  # FULLWIDTH QUESTION MARK
            "Ôºõ": ";",  # FULLWIDTH SEMICOLON
            "\u2000": " ",  # EN QUAD
            "\u2001": " ",  # EM QUAD
            "\u2002": " ",  # EN SPACE
            "\u2003": " ",  # EM SPACE
            "\u2004": " ",  # THREE-PER-EM SPACE
            "\u2005": " ",  # FOUR-PER-EM SPACE
            "\u2006": " ",  # SIX-PER-EM SPACE
            "\u2007": " ",  # FIGURE SPACE
            "\u2008": " ",  # PUNCTUATION SPACE
            "\u2009": " ",  # THIN SPACE
            "\u200a": " ",  # HAIR SPACE
            "\u202f": " ",  # NARROW NO-BREAK SPACE
            "\u205f": " ",  # MEDIUM MATHEMATICAL SPACE
            "\u3000": " ",  # IDEOGRAPHIC SPACE
            "\x00": "",  # NULL
            "\x01": "",  # START OF HEADING
            "\x02": "",  # START OF TEXT
            "\x03": "",  # END OF TEXT
            "\x04": "",  # END OF TRANSMISSION
            "\x05": "",  # ENQUIRY
            "\x06": "",  # ACKNOWLEDGE
            "\x07": "",  # BELL
            "\x08": "",  # BACKSPACE
            "\x0e": "",  # SHIFT OUT
            "\x0f": "",  # SHIFT IN
            "\x10": "",  # DATA LINK ESCAPE
            "\x11": "",  # DEVICE CONTROL ONE
            "\x12": "",  # DEVICE CONTROL TWO
            "\x13": "",  # DEVICE CONTROL THREE
            "\x14": "",  # DEVICE CONTROL FOUR
            "\x15": "",  # NEGATIVE ACKNOWLEDGE
            "\x16": "",  # SYNCHRONOUS IDLE
            "\x17": "",  # END OF TRANSMISSION BLOCK
            "\x18": "",  # CANCEL
            "\x19": "",  # END OF MEDIUM
            "\x1a": "",  # SUBSTITUTE
            "\x1b": "",  # ESCAPE
        }

        for char, replacement in replacements.items():
            text = text.replace(char, replacement)

        # Remove caracteres n√£o-ASCII que possam ter sobrado
        text = "".join(char for char in text if ord(char) < 128)

        # Remove m√∫ltiplos espa√ßos em branco
        text = " ".join(text.split())

        # Remove espa√ßos antes de pontua√ß√£o
        text = re.sub(r"\s+([.,!?;:])", r"\1", text)

        return text

    def extract_json(self, response: str) -> str:
        """Extrai o JSON v√°lido da resposta."""
        # Remove markdown se presente
        if "```json" in response:
            response = response.split("```json")[-1]
            if "```" in response:
                response = response.split("```")[0]

        # Tenta encontrar o objeto JSON completo
        try:
            # Procura por { no in√≠cio e } no fim
            start = response.find("{")
            end = response.rfind("}")
            if start != -1 and end != -1:
                json_str = response[start : end + 1]
                # Tenta validar se √© JSON v√°lido
                json.loads(json_str)
                return json_str
        except json.JSONDecodeError:
            # Se falhou, tenta limpar caracteres problem√°ticos
            cleaned = response.replace("\n", " ").replace("\r", " ")
            cleaned = re.sub(r"\s+", " ", cleaned)

            # Tenta novamente encontrar JSON v√°lido
            try:
                start = cleaned.find("{")
                end = cleaned.rfind("}")
                if start != -1 and end != -1:
                    json_str = cleaned[start : end + 1]
                    json.loads(json_str)  # Valida
                    return json_str
            except json.JSONDecodeError:
                pass

        # Se n√£o conseguiu extrair JSON v√°lido, retorna a resposta limpa
        return response.strip()

    def process_batches(
        self, chain, tickets: list, token_limit: int = 50000
    ) -> Tuple[list, int, int]:
        """Processa os tickets em batches."""
        results = []
        batch_count = 0

        # Buffer para acumular tickets
        batch_tickets = []
        batch_tokens = 0
        header = "Tickets:\n"
        header_tokens = self.estimate_tokens(header)
        batch_tokens += header_tokens

        # Estima total de batches
        estimated_total_batches = (
            sum(len(t["text"]) // 4 for t in tickets) // token_limit
        ) + 1
        print(f"\nTotal estimado de batches: {estimated_total_batches}")

        def process_batch(tickets_text, retry_count=0):
            max_retries = 3
            try:
                input_tokens = self.estimate_tokens(tickets_text)
                self.total_input_tokens += input_tokens

                response = chain.invoke({"tickets_text": tickets_text})
                output_tokens = self.estimate_tokens(response)
                self.total_output_tokens += output_tokens

                print(
                    f"Tokens estimados neste batch - Input: {input_tokens:,}, Output: {output_tokens:,}"
                )

                json_str = self.extract_json(response)
                try:
                    batch_result = json.loads(json_str)

                    # Adiciona logs para rastrear tickets processados vs retornados
                    tickets_no_batch = (
                        len(tickets_text.split("Ticket ")) - 1
                    )  # -1 para compensar o header
                    tickets_processados = len(batch_result)

                    if tickets_no_batch != tickets_processados:
                        print(
                            f"\nAten√ß√£o: Batch processou {tickets_processados} de {tickets_no_batch} tickets"
                        )
                        # Identifica quais tickets n√£o foram processados
                        ticket_ids_input = re.findall(r"Ticket (\d+):", tickets_text)
                        ticket_ids_output = [r.get("ticket_id") for r in batch_result]
                        missing_tickets = set(ticket_ids_input) - set(ticket_ids_output)
                        print(f"Tickets n√£o processados neste batch: {missing_tickets}")

                    # Valida a estrutura do resultado
                    valid_results = []
                    for item in batch_result:
                        if not isinstance(item, dict):
                            print(f"Aviso: Item inv√°lido no resultado: {item}")
                            continue
                        if "ticket_id" not in item:
                            print(f"Aviso: Item sem ticket_id: {item}")
                            continue
                        if not item.get("categorias") or not item.get("resumo"):
                            print(f"Aviso: Item sem categorias ou resumo: {item}")
                            continue
                        valid_results.append(item)

                    if len(valid_results) != len(batch_result):
                        print(
                            f"Aviso: {len(batch_result) - len(valid_results)} resultados foram filtrados por serem inv√°lidos"
                        )

                    return valid_results

                except json.JSONDecodeError as json_error:
                    # Identifica o contexto do erro
                    error_position = json_error.pos
                    context_start = max(0, error_position - 100)
                    context_end = min(len(json_str), error_position + 100)
                    error_context = json_str[context_start:context_end]

                    # Salva o log com informa√ß√µes detalhadas
                    log_file = self.database_dir / f"error_log_{int(time.time())}.log"
                    with open(log_file, "w", encoding="utf-8") as f:
                        f.write("=== Erro de Processamento ===\n")
                        f.write(f"Timestamp: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
                        f.write(f"Tentativa: {retry_count + 1} de {max_retries}\n")
                        f.write(f"Erro: {str(json_error)}\n")
                        f.write("\n=== Contexto do Erro ===\n")
                        f.write(f"Posi√ß√£o do erro: {error_position}\n")
                        f.write(f"Trecho problem√°tico:\n{error_context}\n")
                        f.write("\n=== Resposta Completa da LLM ===\n")
                        f.write(response)
                        f.write("\n\n=== JSON Extra√≠do ===\n")
                        f.write(json_str)
                        f.write("\n\n=== Input do Batch ===\n")
                        f.write(tickets_text)

                    print(f"Log detalhado do erro salvo em: {log_file}")
                    raise

            except Exception as e:
                # Salva o log quando houver erro
                log_file = self.database_dir / f"error_log_{int(time.time())}.log"
                with open(log_file, "w", encoding="utf-8") as f:
                    f.write("=== Erro de Processamento ===\n")
                    f.write(f"Timestamp: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
                    f.write(f"Tentativa: {retry_count + 1} de {max_retries}\n")
                    f.write(f"Erro: {str(e)}\n")
                    f.write(f"Tipo do erro: {type(e)}\n")

                    if isinstance(e, json.JSONDecodeError):
                        f.write("\n=== Detalhes do Erro JSON ===\n")
                        f.write(f"Posi√ß√£o do erro: {e.pos}\n")
                        f.write(f"Linha: {e.lineno}, Coluna: {e.colno}\n")
                        context_start = max(0, e.pos - 100)
                        context_end = min(len(e.doc), e.pos + 100)
                        f.write(
                            f"Contexto do erro:\n{e.doc[context_start:context_end]}\n"
                        )

                    f.write("\n=== Resposta da LLM ===\n")
                    if "response" in locals():
                        f.write(str(response))
                    else:
                        f.write("(Erro ocorreu antes de obter resposta)")

                    f.write("\n\n=== Input do Batch ===\n")
                    f.write(tickets_text)

                print(f"Log detalhado do erro salvo em: {log_file}")

                if retry_count < max_retries:
                    print(
                        f"Erro no processamento, tentativa {retry_count + 1} de {max_retries}"
                    )
                    time.sleep(1)
                    return process_batch(tickets_text, retry_count + 1)
                else:
                    print(f"Erro ap√≥s {max_retries} tentativas:", str(e))
                    return []

        for ticket in tickets:
            batch_tickets.append(ticket["text"])
            batch_tokens += self.estimate_tokens(ticket["text"])

            if (
                batch_tokens >= token_limit
                or len(batch_tickets) >= self.max_tickets_per_batch
            ):
                batch_result = process_batch(" ".join(batch_tickets))
                if batch_result:
                    results.append(batch_result)
                batch_tickets = []
                batch_tokens = 0
                batch_count += 1
                print(f"Batch {batch_count} processado")

        if batch_tickets:
            batch_result = process_batch(" ".join(batch_tickets))
            if batch_result:
                results.append(batch_result)
            batch_count += 1
            print(f"Batch {batch_count} processado")

        return results, batch_count, self.total_input_tokens, self.total_output_tokens

    def _generate_cache_key(self, data: Any) -> str:
        """Gera uma chave de cache baseada nos dados de entrada."""
        # Converte os dados para string e gera um hash
        if isinstance(data, dict):
            # Ordena as chaves para garantir consist√™ncia
            data_str = json.dumps(data, sort_keys=True)
        elif isinstance(data, list):
            # Para listas, converte cada item para string e junta
            data_str = json.dumps([str(item) for item in data], sort_keys=True)
        else:
            # Para outros tipos, converte diretamente para string
            data_str = str(data)

        # Gera o hash SHA-256
        return hashlib.sha256(data_str.encode("utf-8")).hexdigest()

    def _get_from_cache(self, cache_key: str) -> Any:
        """Recupera dados do cache se existirem."""
        if not self.use_cache:
            return None

        cache_file = self.cache_dir / f"{cache_key}.pkl"
        if cache_file.exists():
            try:
                with open(cache_file, "rb") as f:
                    return pickle.load(f)
            except Exception as e:
                print(f"Erro ao carregar cache: {str(e)}")
                return None
        return None

    def _save_to_cache(self, cache_key: str, data: Any) -> None:
        """Salva dados no cache."""
        if not self.use_cache:
            return

        cache_file = self.cache_dir / f"{cache_key}.pkl"
        try:
            with open(cache_file, "wb") as f:
                pickle.dump(data, f)
        except Exception as e:
            print(f"Erro ao salvar cache: {str(e)}")

    def prepare_data(self, input_file: Path, nrows: int = None) -> List[dict]:
        """Prepara os dados do arquivo de entrada com suporte a cache e valida√ß√£o aprimorada."""
        # Gera chave de cache baseada no arquivo e n√∫mero de linhas
        cache_key = self._generate_cache_key(
            {"file": str(input_file), "nrows": nrows, "method": "prepare_data"}
        )

        # Tenta recuperar do cache
        cached_data = self._get_from_cache(cache_key)
        if cached_data is not None:
            print(f"Usando dados em cache para {input_file}")
            return cached_data

        # Se n√£o estiver em cache, processa normalmente
        print(f"Processando dados de {input_file} (n√£o encontrado em cache)")

        # Carrega dados com tratamento robusto de arquivos
        df = self._load_file_robust(input_file, nrows)
        raw_count = len(df)
        print(f"üìä Dados brutos carregados: {raw_count:,} registros")

        # Valida√ß√£o e filtragem melhorada por category
        df = self._filter_by_category(df)
        category_filtered_count = len(df)
        print(f"‚úÖ Ap√≥s filtro category='TEXT': {category_filtered_count:,} registros")

        # Limpeza e prepara√ß√£o de campos
        df = self._prepare_fields(df)

        # Filtragem aprimorada de mensagens AI
        df_filtered = self._filter_ai_messages(df)
        ai_filtered_count = len(df_filtered)
        print(f"ü§ñ Ap√≥s remover mensagens AI: {ai_filtered_count:,} registros")

        # Valida√ß√£o de contagem m√≠nima de mensagens
        df_filtered, valid_tickets = self._validate_message_counts(df_filtered)
        final_count = len(df_filtered)
        print(
            f"üí¨ Tickets v√°lidos (2+ USER e 2+ AGENT): {len(valid_tickets):,} tickets, {final_count:,} mensagens"
        )

        # Agrupa e finaliza
        grouped = self._group_by_ticket(df_filtered)
        result = grouped.to_dict(orient="records")

        # Gera relat√≥rio de filtragem
        self._generate_filtering_report(
            raw_count,
            category_filtered_count,
            ai_filtered_count,
            final_count,
            len(valid_tickets),
            len(result),
        )

        # Salva no cache
        self._save_to_cache(cache_key, result)

        return result

    def _load_file_robust(self, input_file: Path, nrows: int = None) -> pd.DataFrame:
        """Carrega arquivo com tratamento robusto para diferentes formatos e encodings."""
        if input_file.suffix.lower() == ".csv":
            print(
                "üìÑ Detectado arquivo CSV, tentando carregar com diferentes configura√ß√µes..."
            )

            # Lista de configura√ß√µes para tentar
            csv_configs = [
                {"encoding": "utf-8-sig", "sep": ";"},
                {"encoding": "utf-8", "sep": ";"},
                {"encoding": "latin-1", "sep": ";"},
                {"encoding": "utf-8-sig", "sep": ","},
                {"encoding": "utf-8", "sep": ","},
                {"encoding": "latin-1", "sep": ","},
            ]

            for i, config in enumerate(csv_configs):
                try:
                    df = pd.read_csv(
                        input_file,
                        nrows=nrows,
                        quotechar='"',
                        escapechar="\\",
                        on_bad_lines="skip",
                        low_memory=False,
                        dtype=str,
                        **config,
                    )
                    print(
                        f"‚úÖ CSV carregado com sucesso: encoding={config['encoding']}, sep='{config['sep']}'"
                    )
                    print(f"   Colunas detectadas: {list(df.columns)}")
                    return df
                except Exception as e:
                    print(f"   Tentativa {i+1} falhou ({config}): {str(e)}")
                    if i == len(csv_configs) - 1:
                        raise Exception(
                            f"N√£o foi poss√≠vel carregar o arquivo CSV ap√≥s {len(csv_configs)} tentativas"
                        )
        else:
            print("üìä Detectado arquivo Excel, usando pd.read_excel()")
            try:
                df = pd.read_excel(input_file, nrows=nrows, dtype=str)
                print("‚úÖ Excel carregado com sucesso")
                print(f"   Colunas detectadas: {list(df.columns)}")
                return df
            except Exception as e:
                raise Exception(f"Erro ao carregar arquivo Excel: {str(e)}")

    def _filter_by_category(self, df: pd.DataFrame) -> pd.DataFrame:
        """Filtra registros por category='TEXT' com valida√ß√£o robusta case-insensitive."""
        if "category" not in df.columns:
            raise ValueError("Coluna 'category' n√£o encontrada no arquivo")

        # Converte para string e trata valores nulos
        df["category"] = df["category"].fillna("").astype(str)

        # Filtragem case-insensitive mais robusta
        df_filtered = df[df["category"].str.lower().str.strip() == "text"]

        # Log das categorias encontradas para debug
        categories_found = df["category"].value_counts().head(10)
        print("üìã Top 10 categorias encontradas:")
        for cat, count in categories_found.items():
            print(f"   '{cat}': {count:,} registros")

        return df_filtered

    def _prepare_fields(self, df: pd.DataFrame) -> pd.DataFrame:
        """Prepara e limpa campos necess√°rios."""
        required_fields = ["text", "ticket_id", "sender"]
        missing_fields = [field for field in required_fields if field not in df.columns]
        if missing_fields:
            raise ValueError(f"Campos obrigat√≥rios n√£o encontrados: {missing_fields}")

        # Converte e limpa campos
        df["text"] = df["text"].fillna("").astype(str).apply(self.clean_text)
        df["ticket_id"] = df["ticket_id"].fillna("").astype(str)
        df["sender"] = df["sender"].fillna("").astype(str)

        # Remove registros com campos vazios
        initial_count = len(df)
        df = df[
            (df["text"].str.strip() != "")
            & (df["ticket_id"].str.strip() != "")
            & (df["sender"].str.strip() != "")
        ]
        final_count = len(df)

        if initial_count != final_count:
            print(
                f"‚ö†Ô∏è  Removidos {initial_count - final_count} registros com campos vazios"
            )

        return df

    def _filter_ai_messages(self, df: pd.DataFrame) -> pd.DataFrame:
        """Filtra mensagens AI com detec√ß√£o aprimorada de padr√µes."""
        # Padr√µes de sender que indicam mensagens AI
        ai_patterns = [
            "ai",
            "AI",
            "Ai",
            "aI",
            "bot",
            "BOT",
            "Bot",
            "BoT",
            "assistant",
            "ASSISTANT",
            "Assistant",
            "ai_assistant",
            "AI_ASSISTANT",
            "chatbot",
            "CHATBOT",
            "ChatBot",
            "automated",
            "AUTOMATED",
            "Automated",
            "system",
            "SYSTEM",
            "System",
            "auto",
            "AUTO",
            "Auto",
        ]

        # Log das categorias de sender encontradas
        sender_counts = df["sender"].value_counts()
        print("üì® Tipos de sender encontrados:")
        for sender, count in sender_counts.items():
            print(f"   '{sender}': {count:,} mensagens")

        # Filtra usando padr√µes mais robustos
        ai_mask = (
            df["sender"].str.lower().str.strip().isin([p.lower() for p in ai_patterns])
        )
        ai_messages_count = ai_mask.sum()

        df_filtered = df[~ai_mask]

        print(f"ü§ñ Mensagens AI removidas: {ai_messages_count:,}")
        print(f"   Padr√µes detectados: {df[ai_mask]['sender'].unique().tolist()}")

        return df_filtered

    def _validate_message_counts(self, df: pd.DataFrame) -> tuple:
        """Valida contagem m√≠nima de mensagens por ticket com relat√≥rios detalhados."""
        # Conta mensagens por tipo de sender
        user_messages = df[df["sender"] == "USER"]["ticket_id"].value_counts()
        agent_messages = df[df["sender"].isin(["AGENT", "HELPDESK_INTEGRATION"])][
            "ticket_id"
        ].value_counts()

        # Estat√≠sticas detalhadas
        print("üìä Estat√≠sticas de mensagens por ticket:")
        if len(user_messages) > 0:
            print(
                f"   USER - M√©dia: {user_messages.mean():.1f}, Mediana: {user_messages.median():.1f}, M√°x: {user_messages.max()}"
            )
        if len(agent_messages) > 0:
            print(
                f"   AGENT - M√©dia: {agent_messages.mean():.1f}, Mediana: {agent_messages.median():.1f}, M√°x: {agent_messages.max()}"
            )

        # Identifica tickets v√°lidos (pelo menos 2 mensagens de cada tipo)
        valid_user_tickets = set(user_messages[user_messages >= 2].index)
        valid_agent_tickets = set(agent_messages[agent_messages >= 2].index)
        valid_tickets = valid_user_tickets.intersection(valid_agent_tickets)

        print(f"‚úÖ Tickets com 2+ mensagens USER: {len(valid_user_tickets):,}")
        print(f"‚úÖ Tickets com 2+ mensagens AGENT: {len(valid_agent_tickets):,}")
        print(f"‚úÖ Tickets v√°lidos (ambos crit√©rios): {len(valid_tickets):,}")

        # Filtra DataFrame
        df_filtered = df[df["ticket_id"].isin(valid_tickets)]

        return df_filtered, valid_tickets

    def _group_by_ticket(self, df: pd.DataFrame) -> pd.DataFrame:
        """Agrupa mensagens por ticket com preserva√ß√£o de metadados."""
        grouped = (
            df.groupby("ticket_id")
            .agg(
                {
                    "text": lambda x: " ".join(filter(None, x)),
                    "ticket_created_at": "first",
                    "sender": lambda x: f"Total: {len(x)} mensagens ({x.value_counts().to_dict()})",
                }
            )
            .reset_index()
        )

        # Remove tickets sem texto
        initial_count = len(grouped)
        grouped = grouped[grouped["text"].str.strip() != ""]
        final_count = len(grouped)

        if initial_count != final_count:
            print(
                f"‚ö†Ô∏è  Removidos {initial_count - final_count} tickets sem texto v√°lido"
            )

        # Estat√≠sticas finais
        if len(grouped) > 0:
            text_lengths = grouped["text"].str.len()
            print("üìù Estat√≠sticas de texto por ticket:")
            print(f"   Comprimento m√©dio: {text_lengths.mean():.0f} caracteres")
            print(f"   Comprimento mediano: {text_lengths.median():.0f} caracteres")
            print(f"   Comprimento m√°ximo: {text_lengths.max():,} caracteres")

        return grouped

    def _generate_filtering_report(
        self,
        raw_count: int,
        category_count: int,
        ai_count: int,
        final_count: int,
        valid_tickets: int,
        grouped_count: int,
    ):
        """Gera relat√≥rio detalhado do processo de filtragem."""
        print("\n" + "=" * 60)
        print("üìã RELAT√ìRIO DE FILTRAGEM DE DADOS")
        print("=" * 60)
        print(f"Registros brutos:                    {raw_count:,}")
        print(
            f"Ap√≥s filtro category='TEXT':         {category_count:,} ({category_count/raw_count*100:.1f}%)"
        )
        print(
            f"Ap√≥s remo√ß√£o mensagens AI:           {ai_count:,} ({ai_count/raw_count*100:.1f}%)"
        )
        print(
            f"Ap√≥s valida√ß√£o contagem mensagens:   {final_count:,} ({final_count/raw_count*100:.1f}%)"
        )
        print(f"Tickets v√°lidos √∫nicos:              {valid_tickets:,}")
        print(f"Tickets finais agrupados:            {grouped_count:,}")
        print(
            f"\nTaxa de aproveitamento final:        {grouped_count/raw_count*100:.1f}%"
        )
        print("=" * 60)

    def generate_data_quality_report(
        self, tickets: List[dict], output_file: Path = None
    ) -> dict:
        """Gera relat√≥rio detalhado de qualidade dos dados processados."""
        if not tickets:
            print("‚ö†Ô∏è  Nenhum ticket para analisar")
            return {}

        # Converte para DataFrame para an√°lise
        df = pd.DataFrame(tickets)

        # Calcula estat√≠sticas b√°sicas
        ticket_count = len(df)

        # Estat√≠sticas de texto
        text_lengths = df["text"].str.len()
        text_word_counts = df["text"].str.split().str.len()

        # Estat√≠sticas por caracter√≠sticas do ticket
        has_date = "ticket_created_at" in df.columns
        date_stats = {}
        if has_date:
            df["ticket_created_at"] = pd.to_datetime(
                df["ticket_created_at"], errors="coerce"
            )
            date_stats = {
                "data_mais_antiga": df["ticket_created_at"].min(),
                "data_mais_recente": df["ticket_created_at"].max(),
                "periodo_dias": (
                    (df["ticket_created_at"].max() - df["ticket_created_at"].min()).days
                    if pd.notna(df["ticket_created_at"].min())
                    else 0
                ),
            }

        # Compila relat√≥rio
        quality_report = {
            "resumo_geral": {
                "total_tickets": ticket_count,
                "tickets_validos": ticket_count,
                "taxa_sucesso": 100.0,
            },
            "estatisticas_texto": {
                "comprimento_caracteres": {
                    "media": float(text_lengths.mean()),
                    "mediana": float(text_lengths.median()),
                    "minimo": int(text_lengths.min()),
                    "maximo": int(text_lengths.max()),
                    "desvio_padrao": float(text_lengths.std()),
                },
                "contagem_palavras": {
                    "media": float(text_word_counts.mean()),
                    "mediana": float(text_word_counts.median()),
                    "minimo": int(text_word_counts.min()),
                    "maximo": int(text_word_counts.max()),
                    "desvio_padrao": float(text_word_counts.std()),
                },
            },
            "distribuicao_mensagens": self._analyze_message_distribution(df),
            "qualidade_dados": self._assess_data_quality(df),
            "periodo_temporal": date_stats,
        }

        # Gera relat√≥rio visual
        self._print_quality_report(quality_report)

        # Salva em arquivo se especificado
        if output_file:
            import json

            with open(output_file, "w", encoding="utf-8") as f:
                json.dump(quality_report, f, indent=2, ensure_ascii=False, default=str)
            print(f"üìÑ Relat√≥rio salvo em: {output_file}")

        return quality_report

    def _analyze_message_distribution(self, df: pd.DataFrame) -> dict:
        """Analisa distribui√ß√£o de mensagens por ticket."""
        if "sender" in df.columns:
            # Extrai informa√ß√µes de contagem de mensagens do campo sender
            sender_info = df["sender"].str.extract(r"Total: (\d+) mensagens \((.+)\)")
            message_counts = pd.to_numeric(sender_info[0], errors="coerce").dropna()

            if len(message_counts) > 0:
                return {
                    "mensagens_por_ticket": {
                        "media": float(message_counts.mean()),
                        "mediana": float(message_counts.median()),
                        "minimo": int(message_counts.min()),
                        "maximo": int(message_counts.max()),
                        "desvio_padrao": float(message_counts.std()),
                    },
                    "distribuicao_quartis": {
                        "q1": float(message_counts.quantile(0.25)),
                        "q2": float(message_counts.quantile(0.5)),
                        "q3": float(message_counts.quantile(0.75)),
                    },
                }

        return {"erro": "Informa√ß√µes de distribui√ß√£o n√£o dispon√≠veis"}

    def _assess_data_quality(self, df: pd.DataFrame) -> dict:
        """Avalia qualidade geral dos dados."""
        quality_metrics = {
            "completude": {
                "tickets_com_texto": (df["text"].str.strip() != "").sum()
                / len(df)
                * 100,
                "tickets_com_id": (df["ticket_id"].str.strip() != "").sum()
                / len(df)
                * 100,
            },
            "consistencia": {
                "ids_unicos": df["ticket_id"].nunique() == len(df),
                "textos_nao_vazios": (df["text"].str.len() > 10).sum() / len(df) * 100,
            },
            "outliers": {
                "textos_muito_longos": (
                    df["text"].str.len() > df["text"].str.len().quantile(0.95)
                ).sum(),
                "textos_muito_curtos": (
                    df["text"].str.len() < df["text"].str.len().quantile(0.05)
                ).sum(),
            },
        }

        return quality_metrics

    def _print_quality_report(self, report: dict):
        """Imprime relat√≥rio de qualidade de forma organizada."""
        print("\n" + "=" * 80)
        print("üìä RELAT√ìRIO DE QUALIDADE DOS DADOS")
        print("=" * 80)

        # Resumo geral
        resumo = report["resumo_geral"]
        print(f"Total de tickets processados:        {resumo['total_tickets']:,}")
        print(f"Tickets v√°lidos:                     {resumo['tickets_validos']:,}")
        print(f"Taxa de sucesso:                     {resumo['taxa_sucesso']:.1f}%")

        # Estat√≠sticas de texto
        print("\nüìù ESTAT√çSTICAS DE TEXTO:")
        texto = report["estatisticas_texto"]
        chars = texto["comprimento_caracteres"]
        words = texto["contagem_palavras"]

        print(
            f"Caracteres - M√©dia: {chars['media']:.0f}, Mediana: {chars['mediana']:.0f}, Max: {chars['maximo']:,}"
        )
        print(
            f"Palavras   - M√©dia: {words['media']:.1f}, Mediana: {words['mediana']:.1f}, Max: {words['maximo']:,}"
        )

        # Distribui√ß√£o de mensagens
        if "mensagens_por_ticket" in report["distribuicao_mensagens"]:
            print("\nüí¨ DISTRIBUI√á√ÉO DE MENSAGENS:")
            msgs = report["distribuicao_mensagens"]["mensagens_por_ticket"]
            print(
                f"Mensagens por ticket - M√©dia: {msgs['media']:.1f}, Mediana: {msgs['mediana']:.1f}, Max: {msgs['maximo']}"
            )

        # Qualidade dos dados
        print("\n‚úÖ QUALIDADE DOS DADOS:")
        quality = report["qualidade_dados"]
        print(
            f"Completude texto:                    {quality['completude']['tickets_com_texto']:.1f}%"
        )
        print(
            f"Consist√™ncia IDs:                    {'‚úì' if quality['consistencia']['ids_unicos'] else '‚úó'}"
        )
        print(
            f"Textos adequados (>10 chars):        {quality['consistencia']['textos_nao_vazios']:.1f}%"
        )

        # Per√≠odo temporal
        if report["periodo_temporal"]:
            periodo = report["periodo_temporal"]
            print("\nüìÖ PER√çODO TEMPORAL:")
            print(
                f"Per√≠odo analisado:                   {periodo.get('periodo_dias', 0)} dias"
            )
            if "data_mais_antiga" in periodo:
                print(
                    f"Data mais antiga:                    {periodo['data_mais_antiga']}"
                )
                print(
                    f"Data mais recente:                   {periodo['data_mais_recente']}"
                )

        print("=" * 80)

    def process_batches_parallel(
        self, chain, tickets: list, token_limit: int = 50000
    ) -> Tuple[list, int, int, int]:
        """Processa os tickets em batches de forma paralela."""
        # Divide os tickets em batches
        batches = []
        batch_tickets = []
        batch_tokens = 0
        header = "Tickets:\n"
        header_tokens = self.estimate_tokens(header)
        batch_tokens += header_tokens

        # Estima total de batches
        estimated_total_batches = (
            sum(len(t["text"]) // 4 for t in tickets) // token_limit
        ) + 1
        print(f"\nTotal estimado de batches: {estimated_total_batches}")

        # Prepara os batches
        for ticket in tickets:
            ticket_tokens = self.estimate_tokens(ticket["text"])

            # Se adicionar este ticket exceder o limite, finalize o batch atual
            if (
                batch_tokens + ticket_tokens >= token_limit
                or len(batch_tickets) >= self.max_tickets_per_batch
            ):
                if batch_tickets:  # S√≥ adiciona se o batch n√£o estiver vazio
                    batches.append(batch_tickets.copy())
                batch_tickets = []
                batch_tokens = header_tokens

            batch_tickets.append(ticket)
            batch_tokens += ticket_tokens

        # Adiciona o √∫ltimo batch se n√£o estiver vazio
        if batch_tickets:
            batches.append(batch_tickets)

        print(f"Dividido em {len(batches)} batches para processamento paralelo")

        # Fun√ß√£o para processar um √∫nico batch
        def process_single_batch(batch_index, batch):
            try:
                # Constr√≥i o texto para este batch
                batch_text = "\n\n".join(
                    f"Ticket {ticket['ticket_id']}:\n{ticket['text']}"
                    for ticket in batch
                )

                input_tokens = self.estimate_tokens(batch_text)

                # Processa o batch
                response = chain.invoke({"tickets_text": batch_text})
                output_tokens = self.estimate_tokens(response)

                # Extrai e valida o JSON
                json_str = self.extract_json(response)
                batch_result = json.loads(json_str)

                # Valida a estrutura do resultado
                valid_results = []
                for item in batch_result:
                    if not isinstance(item, dict):
                        continue
                    if "ticket_id" not in item:
                        continue
                    if not item.get("categorias") or not item.get("resumo"):
                        continue
                    valid_results.append(item)

                return {
                    "results": valid_results,
                    "input_tokens": input_tokens,
                    "output_tokens": output_tokens,
                    "success": True,
                }
            except Exception as e:
                # Log do erro
                log_file = (
                    self.database_dir
                    / f"error_log_batch_{batch_index}_{int(time.time())}.log"
                )
                with open(log_file, "w", encoding="utf-8") as f:
                    f.write(f"Erro no batch {batch_index}: {str(e)}\n")
                    f.write(f"Tipo do erro: {type(e)}\n")
                    if "response" in locals():
                        f.write(f"Resposta: {response}\n")
                    if "json_str" in locals():
                        f.write(f"JSON extra√≠do: {json_str}\n")

                return {
                    "results": [],
                    "input_tokens": 0,
                    "output_tokens": 0,
                    "success": False,
                    "error": str(e),
                }

        # Processa os batches em paralelo
        total_input_tokens = 0
        total_output_tokens = 0
        batch_results = []

        with concurrent.futures.ThreadPoolExecutor(
            max_workers=self.max_workers
        ) as executor:
            # Submete todos os batches para processamento
            future_to_batch = {
                executor.submit(process_single_batch, i, batch): (i, batch)
                for i, batch in enumerate(batches)
            }

            # Coleta os resultados √† medida que s√£o conclu√≠dos
            for future in tqdm(
                concurrent.futures.as_completed(future_to_batch),
                total=len(batches),
                desc="Processando batches",
            ):
                batch_index, _ = future_to_batch[future]
                try:
                    result = future.result()
                    if result["success"]:
                        batch_results.extend(result["results"])
                        total_input_tokens += result["input_tokens"]
                        total_output_tokens += result["output_tokens"]
                        print(
                            f"Batch {batch_index} processado com sucesso: {len(result['results'])} resultados"
                        )
                    else:
                        print(
                            f"Falha no batch {batch_index}: {result.get('error', 'Erro desconhecido')}"
                        )
                except Exception as e:
                    print(f"Exce√ß√£o ao processar batch {batch_index}: {str(e)}")

        return batch_results, len(batches), total_input_tokens, total_output_tokens

    def process_chunks_with_cache(
        self, chunks: List[Document], map_chain, processor_name: str
    ) -> Tuple[List[str], int, int]:
        """Processa chunks com suporte a cache."""
        partial_results = []
        total_input_tokens = 0
        total_output_tokens = 0

        # Fun√ß√£o para processar um chunk
        def process_chunk(doc_index, doc):
            # Gera chave de cache para este chunk
            chunk_text = doc.page_content
            cache_key = self._generate_cache_key(
                {
                    "chunk": chunk_text[
                        :1000
                    ],  # Usa os primeiros 1000 caracteres para o hash
                    "length": len(chunk_text),
                    "method": f"{processor_name}_chunk",
                }
            )

            # Tenta recuperar do cache
            cached_result = self._get_from_cache(cache_key)
            if cached_result is not None:
                print(f"Usando resultado em cache para chunk {doc_index}")
                return cached_result

            try:
                # Processa o chunk
                input_tokens = self.estimate_tokens(chunk_text)

                result = map_chain.invoke({"text": chunk_text})

                output_tokens = self.estimate_tokens(result)

                # Prepara o resultado
                processed_result = {
                    "result": result,
                    "input_tokens": input_tokens,
                    "output_tokens": output_tokens,
                    "success": True,
                }

                # Salva no cache
                self._save_to_cache(cache_key, processed_result)

                return processed_result
            except Exception as e:
                print(f"Erro ao processar chunk {doc_index}: {str(e)}")
                return {
                    "result": None,
                    "input_tokens": 0,
                    "output_tokens": 0,
                    "success": False,
                    "error": str(e),
                }

        # Processa os chunks em paralelo
        with concurrent.futures.ThreadPoolExecutor(
            max_workers=self.max_workers
        ) as executor:
            # Submete todos os chunks para processamento
            future_to_chunk = {
                executor.submit(process_chunk, i, doc): (i, doc)
                for i, doc in enumerate(chunks, 1)
            }

            # Coleta os resultados √† medida que s√£o conclu√≠dos
            for future in tqdm(
                concurrent.futures.as_completed(future_to_chunk),
                total=len(chunks),
                desc="Processando chunks",
            ):
                chunk_index, _ = future_to_chunk[future]
                try:
                    result = future.result()
                    if result["success"]:
                        partial_results.append(result["result"])
                        total_input_tokens += result["input_tokens"]
                        total_output_tokens += result["output_tokens"]
                        print(
                            f"Chunk {chunk_index}/{len(chunks)} processado - Input: {result['input_tokens']:,}, Output: {result['output_tokens']:,}"
                        )
                    else:
                        print(
                            f"Falha no chunk {chunk_index}: {result.get('error', 'Erro desconhecido')}"
                        )
                except Exception as e:
                    print(f"Exce√ß√£o ao processar chunk {chunk_index}: {str(e)}")

        return partial_results, total_input_tokens, total_output_tokens

    def process_batches_with_cache(
        self, batches: List[List[dict]], process_func, processor_name: str
    ) -> Tuple[List[dict], int, int]:
        """Processa batches com suporte a cache."""
        all_results = []
        total_input_tokens = 0
        total_output_tokens = 0

        # Processa os batches em paralelo
        with concurrent.futures.ThreadPoolExecutor(
            max_workers=self.max_workers
        ) as executor:
            # Submete todos os batches para processamento
            future_to_batch = {
                executor.submit(
                    self._process_batch_with_cache,
                    i,
                    batch,
                    process_func,
                    processor_name,
                ): (i, batch)
                for i, batch in enumerate(batches, 1)
            }

            # Coleta os resultados √† medida que s√£o conclu√≠dos
            for future in tqdm(
                concurrent.futures.as_completed(future_to_batch),
                total=len(batches),
                desc="Processando batches",
            ):
                batch_index, _ = future_to_batch[future]
                try:
                    result = future.result()
                    if result["success"]:
                        all_results.extend(result["results"])
                        total_input_tokens += result["input_tokens"]
                        total_output_tokens += result["output_tokens"]
                        print(
                            f"Batch {batch_index}/{len(batches)} processado: {len(result['results'])} itens"
                        )
                    else:
                        print(
                            f"Falha no batch {batch_index}: {result.get('error', 'Erro desconhecido')}"
                        )
                except Exception as e:
                    print(f"Exce√ß√£o ao processar batch {batch_index}: {str(e)}")

        return all_results, total_input_tokens, total_output_tokens

    def _process_batch_with_cache(
        self, batch_index: int, batch: List[dict], process_func, processor_name: str
    ) -> Dict[str, Any]:
        """Processa um √∫nico batch com suporte a cache."""
        # Gera chave de cache para este batch
        # Usamos os IDs dos tickets como identificador do batch
        ticket_ids = [ticket["ticket_id"] for ticket in batch]
        cache_key = self._generate_cache_key(
            {"ticket_ids": ticket_ids, "method": f"{processor_name}_batch"}
        )

        # Tenta recuperar do cache
        cached_result = self._get_from_cache(cache_key)
        if cached_result is not None:
            print(f"Usando resultado em cache para batch {batch_index}")
            return cached_result

        try:
            # Processa o batch usando a fun√ß√£o fornecida
            result = process_func(batch_index, batch)

            # Salva no cache
            self._save_to_cache(cache_key, result)

            return result
        except Exception as e:
            print(f"Erro ao processar batch {batch_index}: {str(e)}")
            return {
                "results": [],
                "input_tokens": 0,
                "output_tokens": 0,
                "success": False,
                "error": str(e),
            }
