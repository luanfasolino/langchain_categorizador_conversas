# ğŸ“š Exemplos PrÃ¡ticos de Uso

Este documento contÃ©m exemplos reais de como usar o Categorizador de Conversas em diferentes cenÃ¡rios.

## ğŸ¯ CenÃ¡rios de Uso

### 1. ğŸ§ª Primeiro Teste (Dataset Pequeno)

**SituaÃ§Ã£o**: VocÃª quer testar o sistema pela primeira vez

```bash
# Teste com apenas 100 tickets para validar setup
python src/main.py --mode all --nrows 100 --workers 1 --cache-control fresh

# O que esperar:
# âœ… Processamento rÃ¡pido (2-5 minutos)
# âœ… Custo baixo ($0.05-0.20)
# âœ… ValidaÃ§Ã£o de que tudo funciona
```

**SaÃ­da esperada:**
```
ğŸ“Š Dados brutos carregados: 100 registros
ğŸ’¬ Tickets vÃ¡lidos: 15 tickets, 89 mensagens
ğŸ¯ Total de categorias encontradas: 8
ğŸ’° Custo Total: $0.12
```

---

### 2. ğŸš€ ProduÃ§Ã£o (Dataset Completo)

**SituaÃ§Ã£o**: Processar seu dataset completo pela primeira vez

```bash
# ExecuÃ§Ã£o completa com cache fresh
python src/main.py --mode all --cache-control fresh --workers 4

# Monitoramento recomendado:
# ğŸ‘ï¸ Acompanhe os logs para verificar progresso
# ğŸ’° Observe estimativas de custo
# â±ï¸ Tempo estimado aparece no inÃ­cio
```

**Dica**: Para datasets >50K tickets, considere executar em horÃ¡rios de menor uso da API.

---

### 3. âš¡ Re-execuÃ§Ã£o (Com Cache)

**SituaÃ§Ã£o**: VocÃª jÃ¡ processou antes e quer rodar novamente

```bash
# Usa cache existente - muito mais rÃ¡pido
python src/main.py --mode all --cache-control continue

# Economia tÃ­pica:
# âš¡ 70-90% mais rÃ¡pido
# ğŸ’° 70-90% mais barato
# ğŸ¯ Mesma qualidade de resultado
```

---

### 4. ğŸ”„ AtualizaÃ§Ã£o de Dados

**SituaÃ§Ã£o**: VocÃª adicionou novos tickets ao dataset

```bash
# OpÃ§Ã£o 1: Fresh completo (mais lento, mÃ¡xima precisÃ£o)
python src/main.py --mode all --cache-control fresh

# OpÃ§Ã£o 2: Continue (aproveitarÃ¡ cache de dados iguais)
python src/main.py --mode all --cache-control continue

# RecomendaÃ§Ã£o: Use fresh se >30% dos dados mudaram
```

---

### 5. ğŸ›ï¸ Processamento por Etapas

**SituaÃ§Ã£o**: VocÃª quer controle granular do processo

```bash
# Passo 1: SÃ³ categorizaÃ§Ã£o (Map-Reduce-Classify)
python src/main.py --mode categorize --cache-control fresh

# Passo 2: SÃ³ anÃ¡lise e bullet points
python src/main.py --mode summarize --cache-control fresh

# Passo 3: Combinar resultados
python src/main.py --mode merge

# Passo 4: Gerar relatÃ³rios finais
python src/main.py --mode analyze
```

**Vantagem**: Permite analisar resultados intermediÃ¡rios e ajustar se necessÃ¡rio.

---

## ğŸ“Š Exemplos de Resultados

### ğŸ·ï¸ CategorizaÃ§Ã£o TÃ­pica

**Input**: Ticket sobre problema de login
```
Ticket 12345:
USER: NÃ£o consigo fazer login no sistema
AGENT: Qual mensagem de erro aparece?
USER: Diz que senha estÃ¡ incorreta, mas tenho certeza que estÃ¡ certa
AGENT: Vou resetar sua senha. Verifique seu email.
```

**Output**: 
```csv
ticket_id;categoria
12345;Problema de AutenticaÃ§Ã£o
12345;Reset de Senha
12345;Erro de Login
```

### ğŸ’¡ Bullet Points TÃ­picos

**Exemplo de sugestÃµes geradas:**

```csv
bullet
Implementar sistema de recuperaÃ§Ã£o de senha mais intuitivo
Adicionar verificaÃ§Ã£o em duas etapas para maior seguranÃ§a
Criar FAQ especÃ­fico sobre problemas de login mais comuns
Melhorar mensagens de erro para serem mais claras
Implementar bloqueio temporÃ¡rio apÃ³s tentativas falhas
```

### ğŸ“ˆ AnÃ¡lise de Categorias

**Exemplo de relatÃ³rio:**

```
ğŸ” Top 5 Categorias:
1. Problema de Login: 1,245 tickets (23.4%)
2. DÃºvida sobre CobranÃ§a: 987 tickets (18.6%)
3. SolicitaÃ§Ã£o de Reembolso: 654 tickets (12.3%)
4. Problema TÃ©cnico: 543 tickets (10.2%)
5. AlteraÃ§Ã£o de Dados: 432 tickets (8.1%)
```

---

## ğŸ”§ ConfiguraÃ§Ãµes AvanÃ§adas

### âš™ï¸ Ajuste de Performance

```bash
# Para sistemas com pouca memÃ³ria
python src/main.py --mode all --workers 1 --cache-control fresh

# Para mÃ¡xima velocidade (se API permitir)
python src/main.py --mode all --workers 8 --cache-control continue

# Para datasets muito grandes (>100K tickets)
python src/main.py --mode categorize --workers 2 --cache-control fresh
# ... depois continue com summarize, merge, analyze
```

### ğŸ¯ Controle de Qualidade

```bash
# MÃ¡xima precisÃ£o (sempre fresh)
python src/main.py --mode all --cache-control fresh --workers 2

# Teste de consistÃªncia (compare resultados)
python src/main.py --mode categorize --cache-control fresh --input-file "sample1.csv"
python src/main.py --mode categorize --cache-control fresh --input-file "sample2.csv"
```

### ğŸ’° Controle de Custos

```bash
# Para desenvolvimento/teste
python src/main.py --mode all --nrows 500 --cache-control fresh

# Para validaÃ§Ã£o antes de produÃ§Ã£o
python src/main.py --mode all --nrows 5000 --cache-control fresh

# Estimativa antes de execuÃ§Ã£o completa
python src/main.py --mode categorize --nrows 1000 --cache-control fresh
# (use os custos mostrados para extrapolar)
```

---

## ğŸš¨ Troubleshooting Detalhado

### âŒ "Rate Limit Exceeded"

**Sintomas:**
```
âŒ classify_batch_1 falhou: 429 ResourceExhausted
```

**SoluÃ§Ãµes (em ordem de preferÃªncia):**

1. **Reduzir workers:**
```bash
python src/main.py --mode all --workers 1
```

2. **Aumentar intervalo entre requests** (edite cÃ³digo se necessÃ¡rio)

3. **Aguardar e tentar novamente** (rate limits resetam por tempo)

**PrevenÃ§Ã£o:**
- Use no mÃ¡ximo 4 workers
- Evite horÃ¡rios de pico da API (horÃ¡rio comercial US)

---

### âŒ "JSON Parsing Error"

**Sintomas:**
```
âŒ Problemas na resposta: ['JSON parsing error: Expecting value: line 1']
```

**Causas e SoluÃ§Ãµes:**

1. **Batch muito grande:**
```bash
# SoluÃ§Ã£o: CÃ³digo jÃ¡ reduz automaticamente de 50â†’5 tickets por batch
# Se persistir, reduza manualmente no cÃ³digo (categorizer.py linha 126)
```

2. **Resposta truncada da API:**
```bash
# Geralmente resolve sozinho com retry automÃ¡tico
# Se persistir, teste com --nrows menor primeiro
```

**PrevenÃ§Ã£o:**
- Sistema jÃ¡ otimizado para evitar isso
- Batch size ajustado automaticamente

---

### âŒ "Nenhum ticket vÃ¡lido encontrado"

**Sintomas:**
```
ğŸ’¬ Tickets vÃ¡lidos: 0 tickets, 0 mensagens
```

**VerificaÃ§Ãµes:**

1. **Formato do CSV:**
```csv
# âœ… Correto:
ticket_id;category;text;sender;ticket_created_at
12345;TEXT;OlÃ¡ preciso de ajuda;USER;2024-01-15

# âŒ Incorreto:
id,type,message,from,date
12345,text,OlÃ¡ preciso de ajuda,user,2024-01-15
```

2. **ConteÃºdo adequado:**
- Cada ticket precisa de â‰¥2 mensagens USER E â‰¥2 mensagens AGENT
- Campo `category` deve ser 'TEXT' (case insensitive)
- Mensagens nÃ£o podem estar vazias

3. **Verificar relatÃ³rio de filtragem:**
```
ğŸ“‹ RELATÃ“RIO DE FILTRAGEM DE DADOS
Registros brutos: 1,000
ApÃ³s filtro category='TEXT': 953 (95.3%)
ApÃ³s remoÃ§Ã£o mensagens AI: 712 (71.2%)
Tickets vÃ¡lidos Ãºnicos: 37
Taxa de aproveitamento final: 3.7%
```

**Dicas para melhorar aproveitamento:**
- Remova mensagens automÃ¡ticas/bot antes do processamento
- Certifique-se que conversas sÃ£o completas (USER + AGENT)
- Verifique se `sender` estÃ¡ padronizado

---

### âŒ Cache nÃ£o funciona

**Sintomas:**
```
ğŸ“¦ Cache foi utilizado durante o processamento
# Mas pasta database/cache/ continua vazia
```

**JÃ¡ resolvido**: Problema foi corrigido no cÃ³digo. Cache agora funciona perfeitamente.

**VerificaÃ§Ã£o:**
```bash
ls -la database/cache/
# Deve mostrar arquivos .pkl apÃ³s execuÃ§Ã£o
```

---

## ğŸ“ˆ Monitoramento em Tempo Real

### ğŸ‘ï¸ Logs Importantes para Acompanhar

**InÃ­cio do processamento:**
```
ğŸ’° ProjeÃ§Ã£o de custos: $3.45
âš¡ Total estimado de tokens: 2,456,789
ğŸ•’ Tempo estimado: ~2.5 horas
```

**Durante Map phase:**
```
ğŸ”„ Fase MAP: Resumindo problemas dos chunks...
ğŸ“Š Cache performance: 85.2% hit rate
Chunk 145/200 processado - Input: 8,136, Output: 1,526
```

**Durante Classify phase:**
```
ğŸ”„ Processando batches CLASSIFY: 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 34/40 [15:30<02:45, 3.6s/batch]
âœ… Batch 34/40 | Tickets: 5 | Cost: 0.05mÂ¢
```

**ConclusÃ£o:**
```
ğŸ’° RESUMO FINAL:
   â€¢ Total Input Tokens: 2,456,789
   â€¢ Total Output Tokens: 145,234
   â€¢ Custo Total: $3.32
   â€¢ DuraÃ§Ã£o: 2.7h
```

### ğŸ¯ Sinais de Sucesso

- âœ… **Cache hit rate > 0%** (se nÃ£o for primeira execuÃ§Ã£o)
- âœ… **Taxa de sucesso chunks: 100%**
- âœ… **Taxa de sucesso batches: 100%**
- âœ… **Arquivos gerados em database/analysis_reports/**
- âœ… **Custo dentro do esperado**

### ğŸš¨ Sinais de Problema

- âŒ **Muitos errors/retries**
- âŒ **Cache hit rate: 0%** (quando deveria haver cache)
- âŒ **Taxa de sucesso < 95%**
- âŒ **Custo muito acima do projetado**
- âŒ **Tempo muito maior que estimado**

---

## ğŸ“ Dicas de Especialista

### ğŸ† Melhores PrÃ¡ticas

1. **Sempre teste pequeno primeiro:**
```bash
python src/main.py --mode all --nrows 100 --cache-control fresh
```

2. **Use cache inteligentemente:**
   - **Fresh**: Quando dados ou prompts mudaram
   - **Continue**: Para re-execuÃ§Ãµes ou anÃ¡lises adicionais

3. **Monitore custos:**
   - ProjeÃ§Ãµes aparecem no inÃ­cio
   - Custos reais aparecem no final
   - Para datasets grandes, teste 10% primeiro

4. **Workers otimizados:**
   - 1-2 workers: Desenvolvimento/teste
   - 2-4 workers: ProduÃ§Ã£o normal
   - 4+ workers: Apenas se API permitir

### ğŸ’¡ OtimizaÃ§Ãµes AvanÃ§adas

1. **Pre-processamento de dados:**
```bash
# Remova registros desnecessÃ¡rios antes do processamento
# Padronize campo 'sender' 
# Limpe textos muito longos ou muito curtos
```

2. **ExecuÃ§Ã£o em horÃ¡rios otimizados:**
```bash
# APIs tÃªm menos load fora do horÃ¡rio comercial US
# Considere executar Ã  noite ou fim de semana para datasets grandes
```

3. **Backup de resultados:**
```bash
# Sempre faÃ§a backup dos arquivos gerados
cp -r database/analysis_reports/ backup_$(date +%Y%m%d_%H%M%S)/
# Ou backup completo incluindo cache:
cp -r database/ backup_complete_$(date +%Y%m%d_%H%M%S)/
```

---

**ğŸ’ª Com essas prÃ¡ticas, vocÃª terÃ¡ mÃ¡xima eficiÃªncia e qualidade no seu processo de categorizaÃ§Ã£o!**